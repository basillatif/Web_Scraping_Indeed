{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import re\n",
    "import pandas as pd\n",
    "import requests\n",
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.indeed.com/m/jobs?q=%22data+scientist%22&l=Los+Angeles%2C+CA\"\n",
    "response = requests.get(url)\n",
    "\n",
    "#page = urlopen(url)\n",
    "page= response.text\n",
    "soup = BeautifulSoup(page, 'lxml')\n",
    "\n",
    "all_matches = soup.find_all('a', attrs={'rel':['nofollow']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = []\n",
    "company = []\n",
    "location = []\n",
    "jd = []\n",
    "i=0\n",
    "sum_py=0\n",
    "sum_c=0\n",
    "sum_cplus=0\n",
    "sum_java=0\n",
    "sum_js=0\n",
    "sum_r=0\n",
    "sum_sql=0\n",
    "sum_hadoop=0\n",
    "sum_hive=0\n",
    "sum_pig=0\n",
    "sum_spark=0\n",
    "sum_aws=0\n",
    "sum_tab=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LA Job Posting Frequencies for 5 Pages\n",
      "C:  0\n",
      "C++:  4\n",
      "Java:  9\n",
      "JavaScript:  0\n",
      "R:  38\n",
      "Python:  51\n",
      "SQL:  57\n",
      "Hadoop:  9\n",
      "Hive:  4\n",
      "Pig:  1\n",
      "Spark:  15\n",
      "AWS:  39\n",
      "Tableau 19\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    page = urlopen(url)\n",
    "    soup = BeautifulSoup(page, 'lxml')\n",
    "    all_matches = soup.findAll(attrs={'rel':['nofollow']})\n",
    "    for each in all_matches:\n",
    "        jd_url= 'http://www.indeed.com/m/'+each['href']\n",
    "        #jd_page = urlopen(jd_url)\n",
    "        response = requests.get(jd_url)\n",
    "        jd_page= response.text\n",
    "        jd_soup = BeautifulSoup(jd_page, 'lxml')\n",
    "        jd_desc = jd_soup.findAll('div',attrs={'id':['desc']}) ## find the structure like: <div id=\"desc\"></> #break\n",
    "        title.append(jd_soup.body.p.b.font.text)\n",
    "        company.append(jd_desc[0].span.text)\n",
    "        location.append(jd_soup.body.p.span.text)\n",
    "        jd.append(jd_desc[0].text)\n",
    "        c_lang = re.findall(r'[\\b\\/\\s]C\\b[\\s\\,]', str(jd_desc))\n",
    "        c_plus = re.findall(r'[\\b\\/\\s,]?C\\+\\+', str(jd_desc))\n",
    "        java = re.findall(r'[Jj]ava\\b[\\s\\,]', str(jd_desc))\n",
    "        javascript = re.findall(r'Java[Ss]cript', str(jd_desc))\n",
    "        python = re.findall(r'[\\/\\b]?[Pp]ython[\\s\\,\\/]?', str(jd_desc))\n",
    "        r_lang = re.findall(r'[\\b\\/\\s]?R[\\b\\s\\,]', str(jd_desc))\n",
    "        sql = re.findall(r'SQL[\\s,\\/]?', str(jd_desc))\n",
    "        hadoop = re.findall(r'Hadoop[\\s,\\/]?', str(jd_desc))\n",
    "        hive = re.findall(r'Hive[\\s,\\/]?', str(jd_desc))\n",
    "        pig = re.findall(r'Pig[\\s,\\/]?', str(jd_desc))\n",
    "        spark = re.findall(r'Spark[\\s,\\/]?', str(jd_desc))\n",
    "        aws = re.findall(r'AWS[\\s,\\/]?', str(jd_desc))\n",
    "        tab = re.findall(r'Tableau[\\s,\\/]?', str(jd_desc))\n",
    "        sum_c += len(c_lang)\n",
    "        sum_cplus += len(c_plus)\n",
    "        sum_java += len(java)\n",
    "        sum_js += len(javascript)\n",
    "        sum_r += len(r_lang)\n",
    "        sum_py += len(python)\n",
    "        sum_sql += len(sql)\n",
    "        sum_hadoop += len(hadoop)\n",
    "        sum_hive += len(hive)\n",
    "        sum_pig += len(pig)\n",
    "        sum_spark += len(spark)\n",
    "        sum_aws += len(aws)\n",
    "        sum_tab += len(tab)\n",
    "        url_all = soup.findAll(attrs={'rel':['next']})\n",
    "        url = 'http://www.indeed.com/m/'+ str(url_all[0]['href'])\n",
    "        job = {'title': title,\n",
    "         'company': company,\n",
    "         'location': location,\n",
    "         'Job Description': jd}\n",
    "print(\"LA Job Posting Frequencies for 5 Pages\")\n",
    "print(\"C: \", sum_c)\n",
    "print(\"C++: \", sum_cplus)\n",
    "print(\"Java: \", sum_java)\n",
    "print(\"JavaScript: \", sum_js)\n",
    "print(\"R: \", sum_r)\n",
    "print(\"Python: \", sum_py)\n",
    "print(\"SQL: \", sum_sql)\n",
    "print(\"Hadoop: \", sum_hadoop)\n",
    "print(\"Hive: \", sum_hive)\n",
    "print(\"Pig: \", sum_pig)\n",
    "print(\"Spark: \", sum_spark)\n",
    "print(\"AWS: \", sum_aws)\n",
    "print(\"Tableau\", sum_tab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LA Job Posting Frequencies for 5 Pages\n",
      "C:  15\n",
      "C++:  3\n",
      "Java:  6\n",
      "JavaScript:  0\n",
      "R:  39\n",
      "Python:  53\n",
      "SQL:  56\n",
      "Hadoop:  5\n",
      "Hive:  2\n",
      "Pig:  0\n",
      "Spark:  12\n",
      "AWS:  21\n",
      "Tableau 22\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SF Job Posting Frequencies for 5 Pages\n",
      "C:  0\n",
      "C++:  4\n",
      "Java:  9\n",
      "JavaScript:  0\n",
      "R:  38\n",
      "Python:  51\n",
      "SQL:  57\n",
      "Hadoop:  9\n",
      "Hive:  4\n",
      "Pig:  1\n",
      "Spark:  15\n",
      "AWS:  39\n",
      "Tableau 19\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import re\n",
    "import pandas as pd\n",
    "import requests\n",
    "from urllib.request import urlopen\n",
    "url = \"https://www.indeed.com/jobs?q=data+scientist&l=san+francisco%2C+CAstart=0\"\n",
    "response = requests.get(url)\n",
    "page= response.text\n",
    "soup = BeautifulSoup(page, 'lxml')\n",
    "all_matches = soup.find_all('a', attrs={'rel':['nofollow']})\n",
    "for i in range(5):\n",
    "    page = urlopen(url)\n",
    "    soup = BeautifulSoup(page, 'lxml')\n",
    "    all_matches = soup.findAll(attrs={'rel':['nofollow']})\n",
    "    for each in all_matches:\n",
    "        jd_url= 'http://www.indeed.com/m/'+each['href']\n",
    "        #jd_page = urlopen(jd_url)\n",
    "        response = requests.get(jd_url)\n",
    "        jd_page= response.text\n",
    "        jd_soup = BeautifulSoup(jd_page, 'lxml')\n",
    "        jd_desc = jd_soup.findAll('div',attrs={'id':['desc']}) ## find the structure like: <div id=\"desc\"></> #break\n",
    "        title.append(jd_soup.body.p.b.font.text)\n",
    "        company.append(jd_desc[0].span.text)\n",
    "        location.append(jd_soup.body.p.span.text)\n",
    "        jd.append(jd_desc[0].text)\n",
    "        c_lang = re.findall(r'[\\b\\/\\s]C\\b[\\s\\,]', str(jd_desc))\n",
    "        c_plus = re.findall(r'[\\b\\/\\s,]?C\\+\\+', str(jd_desc))\n",
    "        java = re.findall(r'[Jj]ava\\b[\\s\\,]', str(jd_desc))\n",
    "        javascript = re.findall(r'Java[Ss]cript', str(jd_desc))\n",
    "        python = re.findall(r'[\\/\\b]?[Pp]ython[\\s\\,\\/]?', str(jd_desc))\n",
    "        r_lang = re.findall(r'[\\b\\/\\s]?R[\\b\\s\\,]', str(jd_desc))\n",
    "        sql = re.findall(r'SQL[\\s,\\/]?', str(jd_desc))\n",
    "        hadoop = re.findall(r'Hadoop[\\s,\\/]?', str(jd_desc))\n",
    "        hive = re.findall(r'Hive[\\s,\\/]?', str(jd_desc))\n",
    "        pig = re.findall(r'Pig[\\s,\\/]?', str(jd_desc))\n",
    "        spark = re.findall(r'Spark[\\s,\\/]?', str(jd_desc))\n",
    "        aws = re.findall(r'AWS[\\s,\\/]?', str(jd_desc))\n",
    "        tab = re.findall(r'Tableau[\\s,\\/]?', str(jd_desc))\n",
    "        sum_c += len(c_lang)\n",
    "        sum_cplus += len(c_plus)\n",
    "        sum_java += len(java)\n",
    "        sum_js += len(javascript)\n",
    "        sum_r += len(r_lang)\n",
    "        sum_py += len(python)\n",
    "        sum_sql += len(sql)\n",
    "        sum_hadoop += len(hadoop)\n",
    "        sum_hive += len(hive)\n",
    "        sum_pig += len(pig)\n",
    "        sum_spark += len(spark)\n",
    "        sum_aws += len(aws)\n",
    "        sum_tab += len(tab)\n",
    "        url_all = soup.findAll(attrs={'rel':['next']})\n",
    "        url = 'http://www.indeed.com/m/'+ str(url_all[0]['href'])\n",
    "        job = {'title': title,\n",
    "         'company': company,\n",
    "         'location': location,\n",
    "         'Job Description': jd}\n",
    "print(\"SF Job Posting Frequencies for 5 Pages\")\n",
    "print(\"C: \", sum_c)\n",
    "print(\"C++: \", sum_cplus)\n",
    "print(\"Java: \", sum_java)\n",
    "print(\"JavaScript: \", sum_js)\n",
    "print(\"R: \", sum_r)\n",
    "print(\"Python: \", sum_py)\n",
    "print(\"SQL: \", sum_sql)\n",
    "print(\"Hadoop: \", sum_hadoop)\n",
    "print(\"Hive: \", sum_hive)\n",
    "print(\"Pig: \", sum_pig)\n",
    "print(\"Spark: \", sum_spark)\n",
    "print(\"AWS: \", sum_aws)\n",
    "print(\"Tableau\", sum_tab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NY Job Posting Frequencies for 5 Pages\n",
      "C:  0\n",
      "C++:  4\n",
      "Java:  9\n",
      "JavaScript:  0\n",
      "R:  38\n",
      "Python:  51\n",
      "SQL:  57\n",
      "Hadoop:  9\n",
      "Hive:  4\n",
      "Pig:  1\n",
      "Spark:  15\n",
      "AWS:  39\n",
      "Tableau 19\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import re\n",
    "import pandas as pd\n",
    "import requests\n",
    "from urllib.request import urlopen\n",
    "url = \"https://www.indeed.com/jobs?q=data+scientist&l=new+yorkstart=0\"\n",
    "response = requests.get(url)\n",
    "page= response.text\n",
    "soup = BeautifulSoup(page, 'lxml')\n",
    "all_matches = soup.find_all('a', attrs={'rel':['nofollow']})\n",
    "for i in range(5):\n",
    "    page = urlopen(url)\n",
    "    soup = BeautifulSoup(page, 'lxml')\n",
    "    all_matches = soup.findAll(attrs={'rel':['nofollow']})\n",
    "    for each in all_matches:\n",
    "        jd_url= 'http://www.indeed.com/m/'+each['href']\n",
    "        #jd_page = urlopen(jd_url)\n",
    "        response = requests.get(jd_url)\n",
    "        jd_page= response.text\n",
    "        jd_soup = BeautifulSoup(jd_page, 'lxml')\n",
    "        jd_desc = jd_soup.findAll('div',attrs={'id':['desc']}) ## find the structure like: <div id=\"desc\"></> #break\n",
    "        title.append(jd_soup.body.p.b.font.text)\n",
    "        company.append(jd_desc[0].span.text)\n",
    "        location.append(jd_soup.body.p.span.text)\n",
    "        jd.append(jd_desc[0].text)\n",
    "        c_lang = re.findall(r'[\\b\\/\\s]?C\\b[\\s\\,]', str(jd_desc))\n",
    "        c_plus = re.findall(r'[\\b\\/\\s,]?C\\+\\+', str(jd_desc))\n",
    "        java = re.findall(r'[Jj]ava\\b[\\s\\,]', str(jd_desc))\n",
    "        javascript = re.findall(r'Java[Ss]cript', str(jd_desc))\n",
    "        python = re.findall(r'[\\/\\b]?[Pp]ython[\\s\\,\\/]?', str(jd_desc))\n",
    "        r_lang = re.findall(r'[\\b\\/\\s]?R[\\b\\s\\,]', str(jd_desc))\n",
    "        sql = re.findall(r'SQL[\\s,\\/]?', str(jd_desc))\n",
    "        hadoop = re.findall(r'Hadoop[\\s,\\/]?', str(jd_desc))\n",
    "        hive = re.findall(r'Hive[\\s,\\/]?', str(jd_desc))\n",
    "        pig = re.findall(r'Pig[\\s,\\/]?', str(jd_desc))\n",
    "        spark = re.findall(r'Spark[\\s,\\/]?', str(jd_desc))\n",
    "        aws = re.findall(r'AWS[\\s,\\/]?', str(jd_desc))\n",
    "        tab = re.findall(r'Tableau[\\s,\\/]?', str(jd_desc))\n",
    "        sum_c += len(c_lang)\n",
    "        sum_cplus += len(c_plus)\n",
    "        sum_java += len(java)\n",
    "        sum_js += len(javascript)\n",
    "        sum_r += len(r_lang)\n",
    "        sum_py += len(python)\n",
    "        sum_sql += len(sql)\n",
    "        sum_hadoop += len(hadoop)\n",
    "        sum_hive += len(hive)\n",
    "        sum_pig += len(pig)\n",
    "        sum_spark += len(spark)\n",
    "        sum_aws += len(aws)\n",
    "        sum_tab += len(tab)\n",
    "        url_all = soup.findAll(attrs={'rel':['next']})\n",
    "        url = 'http://www.indeed.com/m/'+ str(url_all[0]['href'])\n",
    "        job = {'title': title,\n",
    "         'company': company,\n",
    "         'location': location,\n",
    "         'Job Description': jd}\n",
    "print(\"NY Job Posting Frequencies for 5 Pages\")\n",
    "print(\"C: \", sum_c)\n",
    "print(\"C++: \", sum_cplus)\n",
    "print(\"Java: \", sum_java)\n",
    "print(\"JavaScript: \", sum_js)\n",
    "print(\"R: \", sum_r)\n",
    "print(\"Python: \", sum_py)\n",
    "print(\"SQL: \", sum_sql)\n",
    "print(\"Hadoop: \", sum_hadoop)\n",
    "print(\"Hive: \", sum_hive)\n",
    "print(\"Pig: \", sum_pig)\n",
    "print(\"Spark: \", sum_spark)\n",
    "print(\"AWS: \", sum_aws)\n",
    "print(\"Tableau\", sum_tab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boston Job Posting Frequencies for 5 Pages\n",
      "C:  0\n",
      "C++:  4\n",
      "Java:  9\n",
      "JavaScript:  0\n",
      "R:  38\n",
      "Python:  51\n",
      "SQL:  57\n",
      "Hadoop:  9\n",
      "Hive:  4\n",
      "Pig:  1\n",
      "Spark:  15\n",
      "AWS:  39\n",
      "Tableau 19\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import re\n",
    "import pandas as pd\n",
    "import requests\n",
    "from urllib.request import urlopen\n",
    "url = \"https://www.indeed.com/jobs?q=data+scientist&l=bostonstart=0\"\n",
    "response = requests.get(url)\n",
    "page= response.text\n",
    "soup = BeautifulSoup(page, 'lxml')\n",
    "all_matches = soup.find_all('a', attrs={'rel':['nofollow']})\n",
    "for i in range(5):\n",
    "    page = urlopen(url)\n",
    "    soup = BeautifulSoup(page, 'lxml')\n",
    "    all_matches = soup.findAll(attrs={'rel':['nofollow']})\n",
    "    for each in all_matches:\n",
    "        jd_url= 'http://www.indeed.com/m/'+each['href']\n",
    "        #jd_page = urlopen(jd_url)\n",
    "        response = requests.get(jd_url)\n",
    "        jd_page= response.text\n",
    "        jd_soup = BeautifulSoup(jd_page, 'lxml')\n",
    "        jd_desc = jd_soup.findAll('div',attrs={'id':['desc']}) ## find the structure like: <div id=\"desc\"></> #break\n",
    "        title.append(jd_soup.body.p.b.font.text)\n",
    "        company.append(jd_desc[0].span.text)\n",
    "        location.append(jd_soup.body.p.span.text)\n",
    "        jd.append(jd_desc[0].text)\n",
    "        c_lang = re.findall(r'[\\b\\/\\s]C\\b[\\s\\,]', str(jd_desc))\n",
    "        c_plus = re.findall(r'[\\b\\/\\s,]?C\\+\\+', str(jd_desc))\n",
    "        java = re.findall(r'[Jj]ava\\b[\\s\\,]', str(jd_desc))\n",
    "        javascript = re.findall(r'Java[Ss]cript', str(jd_desc))\n",
    "        python = re.findall(r'[\\/\\b]?[Pp]ython[\\s\\,\\/]?', str(jd_desc))\n",
    "        r_lang = re.findall(r'[\\b\\/\\s]?R[\\b\\s\\,]', str(jd_desc))\n",
    "        sql = re.findall(r'SQL[\\s,\\/]?', str(jd_desc))\n",
    "        hadoop = re.findall(r'Hadoop[\\s,\\/]?', str(jd_desc))\n",
    "        hive = re.findall(r'Hive[\\s,\\/]?', str(jd_desc))\n",
    "        pig = re.findall(r'Pig[\\s,\\/]?', str(jd_desc))\n",
    "        spark = re.findall(r'Spark[\\s,\\/]?', str(jd_desc))\n",
    "        aws = re.findall(r'AWS[\\s,\\/]?', str(jd_desc))\n",
    "        tab = re.findall(r'Tableau[\\s,\\/]?', str(jd_desc))\n",
    "        sum_c += len(c_lang)\n",
    "        sum_cplus += len(c_plus)\n",
    "        sum_java += len(java)\n",
    "        sum_js += len(javascript)\n",
    "        sum_r += len(r_lang)\n",
    "        sum_py += len(python)\n",
    "        sum_sql += len(sql)\n",
    "        sum_hadoop += len(hadoop)\n",
    "        sum_hive += len(hive)\n",
    "        sum_pig += len(pig)\n",
    "        sum_spark += len(spark)\n",
    "        sum_aws += len(aws)\n",
    "        sum_tab += len(tab)\n",
    "        url_all = soup.findAll(attrs={'rel':['next']})\n",
    "        url = 'http://www.indeed.com/m/'+ str(url_all[0]['href'])\n",
    "        job = {'title': title,\n",
    "         'company': company,\n",
    "         'location': location,\n",
    "         'Job Description': jd}\n",
    "print(\"Boston Job Posting Frequencies for 5 Pages\")\n",
    "print(\"C: \", sum_c)\n",
    "print(\"C++: \", sum_cplus)\n",
    "print(\"Java: \", sum_java)\n",
    "print(\"JavaScript: \", sum_js)\n",
    "print(\"R: \", sum_r)\n",
    "print(\"Python: \", sum_py)\n",
    "print(\"SQL: \", sum_sql)\n",
    "print(\"Hadoop: \", sum_hadoop)\n",
    "print(\"Hive: \", sum_hive)\n",
    "print(\"Pig: \", sum_pig)\n",
    "print(\"Spark: \", sum_spark)\n",
    "print(\"AWS: \", sum_aws)\n",
    "print(\"Tableau\", sum_tab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chicago Job Posting Frequencies for 5 Pages\n",
      "C:  0\n",
      "C++:  4\n",
      "Java:  9\n",
      "JavaScript:  0\n",
      "R:  38\n",
      "Python:  51\n",
      "SQL:  57\n",
      "Hadoop:  9\n",
      "Hive:  4\n",
      "Pig:  1\n",
      "Spark:  15\n",
      "AWS:  39\n",
      "Tableau 19\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import re\n",
    "import pandas as pd\n",
    "import requests\n",
    "from urllib.request import urlopen\n",
    "url = \"https://www.indeed.com/jobs?q=data+scientist&l=chicagostart=0\"\n",
    "response = requests.get(url)\n",
    "page= response.text\n",
    "soup = BeautifulSoup(page, 'lxml')\n",
    "all_matches = soup.find_all('a', attrs={'rel':['nofollow']})\n",
    "for i in range(5):\n",
    "    page = urlopen(url)\n",
    "    soup = BeautifulSoup(page, 'lxml')\n",
    "    all_matches = soup.findAll(attrs={'rel':['nofollow']})\n",
    "    for each in all_matches:\n",
    "        jd_url= 'http://www.indeed.com/m/'+each['href']\n",
    "        #jd_page = urlopen(jd_url)\n",
    "        response = requests.get(jd_url)\n",
    "        jd_page= response.text\n",
    "        jd_soup = BeautifulSoup(jd_page, 'lxml')\n",
    "        jd_desc = jd_soup.findAll('div',attrs={'id':['desc']}) ## find the structure like: <div id=\"desc\"></> #break\n",
    "        title.append(jd_soup.body.p.b.font.text)\n",
    "        company.append(jd_desc[0].span.text)\n",
    "        location.append(jd_soup.body.p.span.text)\n",
    "        jd.append(jd_desc[0].text)\n",
    "        c_lang = re.findall(r'[\\b\\/\\s]C\\b[\\s\\,]', str(jd_desc))\n",
    "        c_plus = re.findall(r'[\\b\\/\\s,]?C\\+\\+', str(jd_desc))\n",
    "        java = re.findall(r'[Jj]ava\\b[\\s\\,]', str(jd_desc))\n",
    "        javascript = re.findall(r'Java[Ss]cript', str(jd_desc))\n",
    "        python = re.findall(r'[\\/\\b]?[Pp]ython[\\s\\,\\/]?', str(jd_desc))\n",
    "        r_lang = re.findall(r'[\\b\\/\\s]?R[\\b\\s\\,]', str(jd_desc))\n",
    "        sql = re.findall(r'SQL[\\s,\\/]?', str(jd_desc))\n",
    "        hadoop = re.findall(r'Hadoop[\\s,\\/]?', str(jd_desc))\n",
    "        hive = re.findall(r'Hive[\\s,\\/]?', str(jd_desc))\n",
    "        pig = re.findall(r'Pig[\\s,\\/]?', str(jd_desc))\n",
    "        spark = re.findall(r'Spark[\\s,\\/]?', str(jd_desc))\n",
    "        aws = re.findall(r'AWS[\\s,\\/]?', str(jd_desc))\n",
    "        tab = re.findall(r'Tableau[\\s,\\/]?', str(jd_desc))\n",
    "        sum_c += len(c_lang)\n",
    "        sum_cplus += len(c_plus)\n",
    "        sum_java += len(java)\n",
    "        sum_js += len(javascript)\n",
    "        sum_r += len(r_lang)\n",
    "        sum_py += len(python)\n",
    "        sum_sql += len(sql)\n",
    "        sum_hadoop += len(hadoop)\n",
    "        sum_hive += len(hive)\n",
    "        sum_pig += len(pig)\n",
    "        sum_spark += len(spark)\n",
    "        sum_aws += len(aws)\n",
    "        sum_tab += len(tab)\n",
    "        url_all = soup.findAll(attrs={'rel':['next']})\n",
    "        url = 'http://www.indeed.com/m/'+ str(url_all[0]['href'])\n",
    "        job = {'title': title,\n",
    "         'company': company,\n",
    "         'location': location,\n",
    "         'Job Description': jd}\n",
    "print(\"Chicago Job Posting Frequencies for 5 Pages\")\n",
    "print(\"C: \", sum_c)\n",
    "print(\"C++: \", sum_cplus)\n",
    "print(\"Java: \", sum_java)\n",
    "print(\"JavaScript: \", sum_js)\n",
    "print(\"R: \", sum_r)\n",
    "print(\"Python: \", sum_py)\n",
    "print(\"SQL: \", sum_sql)\n",
    "print(\"Hadoop: \", sum_hadoop)\n",
    "print(\"Hive: \", sum_hive)\n",
    "print(\"Pig: \", sum_pig)\n",
    "print(\"Spark: \", sum_spark)\n",
    "print(\"AWS: \", sum_aws)\n",
    "print(\"Tableau\", sum_tab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Austin Job Posting Frequencies for 5 Pages\n",
      "C:  0\n",
      "C++:  4\n",
      "Java:  9\n",
      "JavaScript:  0\n",
      "R:  38\n",
      "Python:  51\n",
      "SQL:  57\n",
      "Hadoop:  9\n",
      "Hive:  4\n",
      "Pig:  1\n",
      "Spark:  15\n",
      "AWS:  39\n",
      "Tableau 19\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import re\n",
    "import pandas as pd\n",
    "import requests\n",
    "from urllib.request import urlopen\n",
    "url = \"https://www.indeed.com/jobs?q=data+scientist&l=Austin%2C+TXstart=0\"\n",
    "response = requests.get(url)\n",
    "page= response.text\n",
    "soup = BeautifulSoup(page, 'lxml')\n",
    "all_matches = soup.find_all('a', attrs={'rel':['nofollow']})\n",
    "for i in range(5):\n",
    "    page = urlopen(url)\n",
    "    soup = BeautifulSoup(page, 'lxml')\n",
    "    all_matches = soup.findAll(attrs={'rel':['nofollow']})\n",
    "    for each in all_matches:\n",
    "        jd_url= 'http://www.indeed.com/m/'+each['href']\n",
    "        #jd_page = urlopen(jd_url)\n",
    "        response = requests.get(jd_url)\n",
    "        jd_page= response.text\n",
    "        jd_soup = BeautifulSoup(jd_page, 'lxml')\n",
    "        jd_desc = jd_soup.findAll('div',attrs={'id':['desc']}) ## find the structure like: <div id=\"desc\"></> #break\n",
    "        title.append(jd_soup.body.p.b.font.text)\n",
    "        company.append(jd_desc[0].span.text)\n",
    "        location.append(jd_soup.body.p.span.text)\n",
    "        jd.append(jd_desc[0].text)\n",
    "        c_lang = re.findall(r'[\\b\\/\\s]?C\\b[\\s\\,]', str(jd_desc))\n",
    "        c_plus = re.findall(r'[\\b\\/\\s,]?C\\+\\+', str(jd_desc))\n",
    "        java = re.findall(r'[Jj]ava\\b[\\s\\,]', str(jd_desc))\n",
    "        javascript = re.findall(r'Java[Ss]cript', str(jd_desc))\n",
    "        python = re.findall(r'[\\/\\b]?[Pp]ython[\\s\\,\\/]?', str(jd_desc))\n",
    "        r_lang = re.findall(r'[\\b\\/\\s]?R[\\b\\s\\,]', str(jd_desc))\n",
    "        sql = re.findall(r'SQL[\\s,\\/]?', str(jd_desc))\n",
    "        hadoop = re.findall(r'Hadoop[\\s,\\/]?', str(jd_desc))\n",
    "        hive = re.findall(r'Hive[\\s,\\/]?', str(jd_desc))\n",
    "        pig = re.findall(r'Pig[\\s,\\/]?', str(jd_desc))\n",
    "        spark = re.findall(r'Spark[\\s,\\/]?', str(jd_desc))\n",
    "        aws = re.findall(r'AWS[\\s,\\/]?', str(jd_desc))\n",
    "        tab = re.findall(r'Tableau[\\s,\\/]?', str(jd_desc))\n",
    "        sum_c += len(c_lang)\n",
    "        sum_cplus += len(c_plus)\n",
    "        sum_java += len(java)\n",
    "        sum_js += len(javascript)\n",
    "        sum_r += len(r_lang)\n",
    "        sum_py += len(python)\n",
    "        sum_sql += len(sql)\n",
    "        sum_hadoop += len(hadoop)\n",
    "        sum_hive += len(hive)\n",
    "        sum_pig += len(pig)\n",
    "        sum_spark += len(spark)\n",
    "        sum_aws += len(aws)\n",
    "        sum_tab += len(tab)\n",
    "        url_all = soup.findAll(attrs={'rel':['next']})\n",
    "        url = 'http://www.indeed.com/m/'+ str(url_all[0]['href'])\n",
    "        job = {'title': title,\n",
    "         'company': company,\n",
    "         'location': location,\n",
    "         'Job Description': jd}\n",
    "print(\"Austin Job Posting Frequencies for 5 Pages\")\n",
    "print(\"C: \", sum_c)\n",
    "print(\"C++: \", sum_cplus)\n",
    "print(\"Java: \", sum_java)\n",
    "print(\"JavaScript: \", sum_js)\n",
    "print(\"R: \", sum_r)\n",
    "print(\"Python: \", sum_py)\n",
    "print(\"SQL: \", sum_sql)\n",
    "print(\"Hadoop: \", sum_hadoop)\n",
    "print(\"Hive: \", sum_hive)\n",
    "print(\"Pig: \", sum_pig)\n",
    "print(\"Spark: \", sum_spark)\n",
    "print(\"AWS: \", sum_aws)\n",
    "print(\"Tableau\", sum_tab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DC Job Posting Frequencies for 5 Pages\n",
      "C:  0\n",
      "C++:  4\n",
      "Java:  9\n",
      "JavaScript:  0\n",
      "R:  38\n",
      "Python:  51\n",
      "SQL:  57\n",
      "Hadoop:  9\n",
      "Hive:  4\n",
      "Pig:  1\n",
      "Spark:  15\n",
      "AWS:  39\n",
      "Tableau 19\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import re\n",
    "import pandas as pd\n",
    "import requests\n",
    "from urllib.request import urlopen\n",
    "url = \"https://www.indeed.com/jobs?q=data+scientist&l=Washington%2C+DCstart=0\"\n",
    "response = requests.get(url)\n",
    "page= response.text\n",
    "soup = BeautifulSoup(page, 'lxml')\n",
    "all_matches = soup.find_all('a', attrs={'rel':['nofollow']})\n",
    "for i in range(5):\n",
    "    page = urlopen(url)\n",
    "    soup = BeautifulSoup(page, 'lxml')\n",
    "    all_matches = soup.findAll(attrs={'rel':['nofollow']})\n",
    "    for each in all_matches:\n",
    "        jd_url= 'http://www.indeed.com/m/'+each['href']\n",
    "        #jd_page = urlopen(jd_url)\n",
    "        response = requests.get(jd_url)\n",
    "        jd_page= response.text\n",
    "        jd_soup = BeautifulSoup(jd_page, 'lxml')\n",
    "        jd_desc = jd_soup.findAll('div',attrs={'id':['desc']}) ## find the structure like: <div id=\"desc\"></> #break\n",
    "        title.append(jd_soup.body.p.b.font.text)\n",
    "        company.append(jd_desc[0].span.text)\n",
    "        location.append(jd_soup.body.p.span.text)\n",
    "        jd.append(jd_desc[0].text)\n",
    "        c_lang = re.findall(r'[\\b\\/\\s]C\\b[\\s\\,]', str(jd_desc))\n",
    "        c_plus = re.findall(r'[\\b\\/\\s,]?C\\+\\+', str(jd_desc))\n",
    "        java = re.findall(r'[Jj]ava\\b[\\s\\,]', str(jd_desc))\n",
    "        javascript = re.findall(r'Java[Ss]cript', str(jd_desc))\n",
    "        python = re.findall(r'[\\/\\b]?[Pp]ython[\\s\\,\\/]?', str(jd_desc))\n",
    "        r_lang = re.findall(r'[\\b\\/\\s]?R[\\b\\s\\,]', str(jd_desc))\n",
    "        sql = re.findall(r'SQL[\\s,\\/]?', str(jd_desc))\n",
    "        hadoop = re.findall(r'Hadoop[\\s,\\/]?', str(jd_desc))\n",
    "        hive = re.findall(r'Hive[\\s,\\/]?', str(jd_desc))\n",
    "        pig = re.findall(r'Pig[\\s,\\/]?', str(jd_desc))\n",
    "        spark = re.findall(r'Spark[\\s,\\/]?', str(jd_desc))\n",
    "        aws = re.findall(r'AWS[\\s,\\/]?', str(jd_desc))\n",
    "        tab = re.findall(r'Tableau[\\s,\\/]?', str(jd_desc))\n",
    "        sum_c += len(c_lang)\n",
    "        sum_cplus += len(c_plus)\n",
    "        sum_java += len(java)\n",
    "        sum_js += len(javascript)\n",
    "        sum_r += len(r_lang)\n",
    "        sum_py += len(python)\n",
    "        sum_sql += len(sql)\n",
    "        sum_hadoop += len(hadoop)\n",
    "        sum_hive += len(hive)\n",
    "        sum_pig += len(pig)\n",
    "        sum_spark += len(spark)\n",
    "        sum_aws += len(aws)\n",
    "        sum_tab += len(tab)\n",
    "        url_all = soup.findAll(attrs={'rel':['next']})\n",
    "        url = 'http://www.indeed.com/m/'+ str(url_all[0]['href'])\n",
    "        job = {'title': title,\n",
    "         'company': company,\n",
    "         'location': location,\n",
    "         'Job Description': jd}\n",
    "print(\"DC Job Posting Frequencies for 5 Pages\")\n",
    "print(\"C: \", sum_c)\n",
    "print(\"C++: \", sum_cplus)\n",
    "print(\"Java: \", sum_java)\n",
    "print(\"JavaScript: \", sum_js)\n",
    "print(\"R: \", sum_r)\n",
    "print(\"Python: \", sum_py)\n",
    "print(\"SQL: \", sum_sql)\n",
    "print(\"Hadoop: \", sum_hadoop)\n",
    "print(\"Hive: \", sum_hive)\n",
    "print(\"Pig: \", sum_pig)\n",
    "print(\"Spark: \", sum_spark)\n",
    "print(\"AWS: \", sum_aws)\n",
    "print(\"Tableau\", sum_tab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
